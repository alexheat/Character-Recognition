{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alex_heaton_p1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexheat/MIDS/blob/master/Digit_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z6UHmLYVhWAN"
      },
      "source": [
        "# Project 1: Digit Classification with KNN and Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "03M_JSg3hWAO"
      },
      "source": [
        "In this project, you'll implement your own image recognition system for classifying digits. Read through the code and the instructions carefully and add your own code where indicated. Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
        "\n",
        "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but <b> please prepare your own write-up (with your own code). </b>\n",
        "\n",
        "If you're interested, check out these links related to digit recognition:\n",
        "\n",
        "* Yann Lecun's MNIST benchmarks: http://yann.lecun.com/exdb/mnist/\n",
        "* Stanford Streetview research and data: http://ufldl.stanford.edu/housenumbers/\n",
        "\n",
        "Finally, if you'd like to get started with Tensorflow, you can read through this tutorial: https://www.tensorflow.org/tutorials/keras/basic_classification. It uses a dataset called \"fashion_mnist\", which is identical in structure to the original digit mnist, but uses images of clothing rather than images of digits. The number of training examples and number of labels is the same. In fact, you can simply replace the code that loads \"fashion_mnist\" with \"mnist\" and everything should work fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJ9ayCvyhWAP",
        "colab": {}
      },
      "source": [
        "# This tells matplotlib not to try opening a new window for each plot.\n",
        "%matplotlib inline\n",
        "\n",
        "# Import a bunch of libraries.\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report \n",
        "\n",
        "# Set the randomizer seed so results are the same each time.\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-BJ6EKJqrSW_",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sO1t0ypThWAR"
      },
      "source": [
        "Load the data. Notice that the data gets partitioned into training, development, and test sets. Also, a small subset of the training data called mini_train_data and mini_train_labels gets defined, which you should use in all the experiments below, unless otherwise noted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3yK9DacchWAS",
        "colab": {}
      },
      "source": [
        "# Load the digit data from https://www.openml.org/d/554 or from default local location '~/scikit_learn_data/...'\n",
        "X, Y = fetch_openml(name='mnist_784', return_X_y=True, cache=False)\n",
        "\n",
        "\n",
        "# Rescale grayscale values to [0,1].\n",
        "X = X / 255.0\n",
        "\n",
        "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this\n",
        "# permutation to X and Y.\n",
        "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
        "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
        "X, Y = X[shuffle], Y[shuffle]\n",
        "\n",
        "print('data shape: ', X.shape)\n",
        "print('label shape:', Y.shape)\n",
        "\n",
        "# Set some variables to hold test, dev, and training data.\n",
        "test_data, test_labels = X[61000:], Y[61000:]\n",
        "dev_data, dev_labels = X[60000:61000], Y[60000:61000]\n",
        "train_data, train_labels = X[:60000], Y[:60000]\n",
        "mini_train_data, mini_train_labels = X[:1000], Y[:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "atc2JpWKhWAV"
      },
      "source": [
        "### Part 1:\n",
        "\n",
        "Show a 10x10 grid that visualizes 10 examples of each digit.\n",
        "\n",
        "Notes:\n",
        "* You can use `plt.rc()` for setting the colormap, for example to black and white.\n",
        "* You can use `plt.subplot()` for creating subplots.\n",
        "* You can use `plt.imshow()` for rendering a matrix.\n",
        "* You can use `np.array.reshape()` for reshaping a 1D feature vector into a 2D matrix (for rendering)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "436UeH7JhWAW",
        "colab": {}
      },
      "source": [
        "#def P1(num_examples=10):\n",
        "\n",
        "### STUDENT START ###\n",
        "\n",
        "#Setup the plot grid and stlying \n",
        "f, axarr = plt.subplots(10, 10,figsize=(7,7))\n",
        "plt.set_cmap('binary')\n",
        "plt.setp(axarr, xticks=[], yticks=[])\n",
        "plt.suptitle('10 Samples of Each Digit (0-9) from MNIST Data Set', y=0.92, fontsize=14)\n",
        "\n",
        "#Iterate through each of the 10 digits\n",
        "for d in range(10):\n",
        "    n = 0\n",
        "    #Iterate through the scanned digits\n",
        "    for i in range(len(mini_train_labels)):\n",
        "        #l stands for lable, which we want as an int\n",
        "        l = int(mini_train_labels[i])\n",
        "        #We want 10 samples of each digit\n",
        "        #If the scanned digit matches the value we want for this row, render that item\n",
        "        if n < 10 and l==d:\n",
        "            axarr[d, n].imshow(mini_train_data[i].reshape(28,28))\n",
        "            n += 1\n",
        "### STUDENT END ###\n",
        "\n",
        "#P1(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EMQAHr7QhWAX"
      },
      "source": [
        "### Part 2:\n",
        "\n",
        "Produce k-Nearest Neighbors models with k $\\in$ [1,3,5,7,9].  Evaluate and show the accuracy of each model. For the 1-Nearest Neighbor model, additionally show the precision, recall, and F1 for each label. Which digit is the most difficult for the 1-Nearest Neighbor model to recognize?\n",
        "\n",
        "Notes:\n",
        "* Train on the mini train set.\n",
        "* Evaluate performance on the dev set.\n",
        "* You can use `KNeighborsClassifier` to produce a k-nearest neighbor model.\n",
        "* You can use `classification_report` to get precision, recall, and F1 results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-it5pn8-hWAY",
        "colab": {}
      },
      "source": [
        "#def P2(k_values):\n",
        "\n",
        "### STUDENT START ###\n",
        "\n",
        "print(\"Evaluate and show the accuracy of each model:\")\n",
        "k_values = [1, 3, 5, 7, 9]\n",
        "accuracies = []\n",
        "for k in k_values:\n",
        "  model = KNeighborsClassifier(n_neighbors=k)\n",
        "  model.fit(mini_train_data, mini_train_labels)\n",
        "  test_predicted_labels = model.predict(dev_data)\n",
        "  accuracies.append(sklearn.metrics.accuracy_score(dev_labels, test_predicted_labels))\n",
        "  print(\"K = \", k, \"Accuracy score:\", accuracies[-1])\n",
        "\n",
        "#Use KNeighborsClassifier to produce a k-nearest neighbor model\n",
        "k1model = KNeighborsClassifier(n_neighbors=1)\n",
        "#Train on the mini train set.\n",
        "k1model.fit(mini_train_data, mini_train_labels)\n",
        "#Evaluate performance on the dev set.\n",
        "test_predicted_labels = k1model.predict(dev_data)\n",
        "\n",
        "report = classification_report(dev_labels, test_predicted_labels, output_dict=False)\n",
        "\n",
        "print(\"\\n\\nOutput performance of K=1 model on each digit:\")\n",
        "print(report)\n",
        "\n",
        "#dev_data, dev_labels = X[60000:61000], Y[60000:61000]\n",
        "\n",
        "### STUDENT END ###\n",
        "\n",
        "#k_values = [1, 3, 5, 7, 9]\n",
        "#P2(k_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tZc9gzn5hWAZ"
      },
      "source": [
        "ANSWER: The model has the most difficulty recognizing the digit 8, which as an f1-score of 0.8. I use the f1-score as the primary metric to evaluate the model. On this [reference page for the classification_report](https://www.scikit-yb.org/en/latest/api/classifier/classification_report.html) it states \n",
        ">As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n",
        "The f1-score considers the number of times that the model is correct when it makes a guess (precision-true positive) as well as the number of times that it fails to predict the digit (recall-false negatives).  The model performs most poorly in terms of recall on the digit 8, meaning it fails to identify images that are actualy 8s. Looking at the samples of 8s in 10x10 table it is not obvious to my as a human why 8s are more difficult to predict, so I will need to analyze further to improve the model. \n",
        "\n",
        "Also, as for the models with different levels of K, the K=1 model performs the best, with an f1 score of 0.88. All the models are very close around 0.87-0.88, but K=1 was the best. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7b6YEAzzhWAa"
      },
      "source": [
        "### Part 3:\n",
        "\n",
        "Produce 1-Nearest Neighbor models using training data of various sizes.  Evaluate and show the performance of each model.  Additionally, show the time needed to measure the performance of each model.\n",
        "\n",
        "Notes:\n",
        "* Train on subsets of the train set.  For each subset, take just the first part of the train set without re-ordering.\n",
        "* Evaluate on the dev set.\n",
        "* You can use `KNeighborsClassifier` to produce a k-nearest neighbor model.\n",
        "* You can use `time.time()` to measure elapsed time of operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gEpNzDEjhWAa",
        "colab": {}
      },
      "source": [
        "#def P3(train_sizes, accuracies):\n",
        "\n",
        "### STUDENT START ###\n",
        "\n",
        "print(\"Evaluate and show the accuracy of each model:\")\n",
        "train_sizes = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600]\n",
        "accuracies = []\n",
        "for s in train_sizes:\n",
        "  train_data, train_labels = X[:s], Y[:s]\n",
        "  model = KNeighborsClassifier(n_neighbors=1)\n",
        "  start_time_train = time.time()\n",
        "  model.fit(train_data, train_labels)\n",
        "  end_time_train = time.time()\n",
        "\n",
        "  start_time_predict = time.time()\n",
        "  test_predicted_labels = model.predict(dev_data)\n",
        "  end_time_predict = time.time()\n",
        "  train_time = end_time_train - start_time_train\n",
        "  predict_time = end_time_predict - start_time_predict\n",
        "\n",
        "  accuracies.append(sklearn.metrics.accuracy_score(dev_labels, test_predicted_labels))\n",
        "  report_dict = classification_report(dev_labels, test_predicted_labels, output_dict=True)\n",
        "  print(\"Train size = \", s)\n",
        "  print(\"\\t\", \"Accuracy score:\", accuracies[-1])\n",
        "  print(\"\\t\", report_dict['weighted avg'])\n",
        "  print(\"\\t\", \"Train time:\", round(train_time, 2))\n",
        "  print(\"\\t\", \"Predict time:\", round(predict_time, 2))\n",
        "\n",
        "### STUDENT END ###\n",
        "\n",
        "#train_sizes = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600]\n",
        "#accuracies = []\n",
        "#P3(train_sizes, accuracies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B56lVsKNhWAc"
      },
      "source": [
        "# Part 4:\n",
        "\n",
        "Produce a linear regression model that predicts accuracy of a 1-Nearest Neighbor model given training set size. Show $R^2$ of the linear regression model.  Show the accuracies predicted for training set sizes 60000, 120000, and 1000000.  Show a lineplot of actual accuracies and predicted accuracies vs. training set size over the range of training set sizes in the training data.  What's wrong with using linear regression here?\n",
        "\n",
        "Apply a transformation to the predictor features and a transformation to the outcome that make the predictions more reasonable.  Show $R^2$ of the improved linear regression model.  Show the accuracies predicted for training set sizes 60000, 120000, and 1000000.  Show a lineplot of actual accuracies and predicted accuracies vs. training set size over the range of training set sizes in the training data - be sure to display accuracies and training set sizes in appropriate units.\n",
        "\n",
        "\n",
        "Notes:\n",
        "* Train the linear regression models on all of the (transformed) accuracies estimated in Problem 3.\n",
        "* Evaluate the linear regression models on all of the (transformed) accuracies estimated in Problem 3.\n",
        "* You can use `LinearRegression` to produce a linear regression model.\n",
        "* Remember that the sklearn `fit()` functions take an input matrix X and output vector Y. So, each input example in X is a vector, even if it contains only a single value.\n",
        "* Hint re: predictor feature transform: Accuracy increases with training set size logarithmically.\n",
        "* Hint re: outcome transform: When y is a number in range 0 to 1, then odds(y)=y/(1-y) is a number in range 0 to infinity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4xE_qIJghWAc",
        "colab": {}
      },
      "source": [
        "#def P4():\n",
        "\n",
        "### STUDENT START ###\n",
        "#Must run the cell above to get some of the values\n",
        "\n",
        "# Produce a linear regression model that predicts accuracy of a \n",
        "# 1-Nearest Neighbor model given training set size.\n",
        "lmodel = LinearRegression()\n",
        "train_sizes = np.array(train_sizes).reshape(-1, 1)\n",
        "train_accuracies =  np.array(accuracies).reshape(-1, 1)\n",
        "lmodel.fit(train_sizes, train_accuracies)\n",
        "\n",
        "print(\"Show  r2  of the linear regression model:\")\n",
        "print(\"R2 score:\",   round(lmodel.score(train_sizes, train_accuracies), 2,),\"\\b\")\n",
        "\n",
        "\n",
        "print(\"Show the accuracies predicted for training set sizes 60000, 120000, and 1000000.\")\n",
        "test_sizes = np.array([60000, 120000, 1000000]).reshape(-1, 1)\n",
        "test_predicted_accuracies = lmodel.predict(test_sizes)\n",
        "print(test_predicted_accuracies)\n",
        "\n",
        "#Get predicted and actual for all training set sizes\n",
        "#First get all the accuracies \n",
        "all_train_sizes = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 60000, 120000, 1000000]\n",
        "all_accuracies = []\n",
        "for s in all_train_sizes: \n",
        "  train_data, train_labels = X[:s], Y[:s]\n",
        "  model = KNeighborsClassifier(n_neighbors=1)\n",
        "  model.fit(train_data, train_labels)\n",
        "  test_predicted_labels = model.predict(dev_data)\n",
        "  all_accuracies.append(sklearn.metrics.accuracy_score(dev_labels, test_predicted_labels))\n",
        "\n",
        "#Get all of the predicted values\n",
        "all_test_sizes = np.array(all_train_sizes).reshape(-1, 1)\n",
        "test_predicted_accuracies = lmodel.predict(all_test_sizes)\n",
        "\n",
        "#Create visual\n",
        "string_of_sizes = [\"100\", \"200\", \"400\", \"800\", \"1600\", \"3200\", \"6400\", \"12800\", \"25600\", \"60000\", \"120000\", \"1000000\"]\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title(\"Figure A: Predicted vs Actual Accuracies\")\n",
        "plt.plot(string_of_sizes, all_accuracies, label='actual')\n",
        "plt.plot(string_of_sizes, test_predicted_accuracies, label='predicted')\n",
        "plt.ylim(0, 8)\n",
        "plt.xlabel(\"Number of training samples\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "#Apply a transformation to the predictor features and a transformation to the outcome that make the predictions more reasonable.  \n",
        "#Trainform log+1 to get range from 0-1\n",
        "train_accuracies_log = np.log(all_accuracies[:9]) + 1\n",
        "train_sizes_log = np.log(all_test_sizes[:9])\n",
        "lm_log_model = LinearRegression()\n",
        "lm_log_model.fit(train_sizes_log, train_accuracies_log)\n",
        "print(\"R2 of the improved linear regression model:\", round(lm_log_model.score(train_sizes_log, train_accuracies_log),2))\n",
        "\n",
        "#Show the accuracies predicted for training set sizes 60000, 120000, and 1000000. \n",
        "test_sizes_log = np.log(all_test_sizes[9:])\n",
        "print(\"Transformed sizes:\", test_sizes_log)\n",
        "print(\"Predicted accuracies:\", list(lm_log_model.predict(test_sizes_log)))\n",
        "print(\"The predicted accuracies stay close to an upper range of 1\")\n",
        "\n",
        "#Show a lineplot of actual accuracies and predicted accuracies vs. \n",
        "#training set size over the range of training set sizes in the training data - \n",
        "#be sure to display accuracies and training set sizes in appropriate units.\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title(\"Figure B: Predicted vs Actual Accuracies of Transformed Inputs\")\n",
        "plt.plot(np.log(all_test_sizes), np.log(all_accuracies) +1, label='actual')\n",
        "plt.plot(np.log(all_test_sizes), lm_log_model.predict(np.log(all_test_sizes)), label='predicted')\n",
        "plt.xlabel(\"Log of number of training samples\")\n",
        "plt.ylabel(\"Accuracy (Log of Accuracy+1)\")\n",
        "plt.legend()\n",
        "\n",
        "### STUDENT END ###\n",
        "#test_sizes = [60000, 120000, 1000000]\n",
        "#lm_log_model = model.predict(test_sizes)\n",
        "#P4()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HYYYL9cGhWAe"
      },
      "source": [
        " **What's wrong with using linear regression here?**\n",
        "ANSWER: Linear regression assumes that the accuracy will keep growing in a strait line (Figure A). But this cannot be true because the accuracy has a maximum value of 100. Transforming the variables flattens the slope of the line so that stays closer to 1 (Figure B). Though even with a transoformation my model is flawed because it does exceed 1 because it is a straight line and as X goes higher y will always go higher because of the linear relationship. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "geAQJjGRhWAe"
      },
      "source": [
        "### Part 5:\n",
        "\n",
        "Produce a 1-Nearest Neighbor model and show the confusion matrix. Which pair of digits does the model confuse most often? Show the images of these most often confused digits.\n",
        "\n",
        "Notes:\n",
        "- Train on the mini train set.\n",
        "- Evaluate performance on the dev set.\n",
        "- You can use `confusion_matrix()` to produce a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bq36xaQohWAf",
        "colab": {}
      },
      "source": [
        "#def P5():\n",
        "\n",
        "### STUDENT START ###\n",
        "#Produce a 1-Nearest Neighbor model and show the confusion matrix. \n",
        "model = KNeighborsClassifier(n_neighbors=1)\n",
        "model.fit(mini_train_data, mini_train_labels)\n",
        "test_predicted_labels = model.predict(dev_data)\n",
        "cm = confusion_matrix(dev_labels, test_predicted_labels, labels=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)\n",
        "\n",
        "#Find the miscassified numbers and store them in arrays \n",
        "misclass4s = []\n",
        "misclass9s = []\n",
        "for i in range(len(dev_labels)):\n",
        "  #Find 4 misclassified as 9s\n",
        "  if (dev_labels[i] == \"4\") & (test_predicted_labels[i] == \"9\"):\n",
        "    misclass4s.append(i)\n",
        "  #Find 4 misclassified as 9s\n",
        "  if (dev_labels[i] == \"9\") & (test_predicted_labels[i] == \"4\"):\n",
        "    misclass9s.append(i)\n",
        "  ### STUDENT END ###\n",
        "\n",
        "fig=plt.figure(figsize=(7, 7))\n",
        "plt.set_cmap('binary')\n",
        "plt.suptitle('4s mis-classified as 9s', y=.58, fontsize=14)\n",
        "\n",
        "columns = 11\n",
        "rows = 1\n",
        "for i in range(1, columns*rows +1):\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(dev_data[misclass4s[i-1]].reshape(28,28))\n",
        "    plt.xticks([]),plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "fig=plt.figure(figsize=(2, 2))\n",
        "plt.set_cmap('binary')\n",
        "plt.suptitle('9s mis-classified as 4s', y=0.75, fontsize=14)\n",
        "\n",
        "columns = 3\n",
        "rows = 1\n",
        "for i in range(1, columns*rows +1):\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(dev_data[misclass9s[i-1]].reshape(28,28))\n",
        "    plt.xticks([]),plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "#P5()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lkdGCYlOrSXg"
      },
      "source": [
        "ANSWER: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tgqMKb-hhWAh"
      },
      "source": [
        "### Part 6:\n",
        "\n",
        "A common image processing technique is to smooth an image by blurring. The idea is that the value of a particular pixel is estimated as the weighted combination of the original value and the values around it. Typically, the blurring is Gaussian, i.e., the weight of a pixel's influence is determined by a Gaussian function over the distance to the relevant pixel.\n",
        "\n",
        "Implement a simplified Gaussian blur filter by just using the 8 neighboring pixels like this: the smoothed value of a pixel is a weighted combination of the original value and the 8 neighboring values.\n",
        "\n",
        "Pick a weight, then produce and evaluate four 1-Nearest Neighbor models by applying your blur filter in these ways:\n",
        "- Do not use the filter\n",
        "- Filter the training data but not the dev data\n",
        "- Filter the dev data but not the training data\n",
        "- Filter both training data and dev data\n",
        "\n",
        "Show the accuracies of the four models evaluated as described.  Try to pick a weight that makes one model's accuracy at least 0.9.\n",
        "\n",
        "Notes:\n",
        "* Train on the (filtered) mini train set.\n",
        "* Evaluate performance on the (filtered) dev set.\n",
        "* There are other Guassian blur filters available, for example in `scipy.ndimage.filters`. You are welcome to experiment with those, but you are likely to get the best results with the simplified version described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lSKHmHGshWAi",
        "colab": {}
      },
      "source": [
        "#def P6():\n",
        "    \n",
        "### STUDENT START ###\n",
        "\n",
        "import scipy.misc\n",
        "from scipy import ndimage\n",
        "\n",
        "#Apply blur to data dets\n",
        "sigma_val=1.365\n",
        "#mini_train_data_blur = np.array([ndimage.gaussian_filter(x, sigma=sigma_val) for x in mini_train_data])\n",
        "#dev_data_blur = np.array([ndimage.gaussian_filter(x, sigma=sigma_val) for x in dev_data])\n",
        "\n",
        "mini_train_data_blur = np.array([ndimage.gaussian_filter(x, sigma=sigma_val) for x in mini_train_data])\n",
        "dev_data_blur = np.array([ndimage.gaussian_filter(x, sigma=sigma_val) for x in dev_data])\n",
        "\n",
        "\n",
        "print(\"Do not use the filter:\")\n",
        "model = KNeighborsClassifier(n_neighbors=1)\n",
        "model.fit(mini_train_data, mini_train_labels)\n",
        "test_predicted_labels = model.predict(dev_data)\n",
        "print(\"Acuracy = \", sklearn.metrics.accuracy_score(dev_labels, test_predicted_labels) , \"\\n\")\n",
        "\n",
        "print(\"Filter the training data but not the dev data:\")\n",
        "model = KNeighborsClassifier(n_neighbors=1)\n",
        "model.fit(mini_train_data_blur, mini_train_labels)\n",
        "test_predicted_labels = model.predict(dev_data)\n",
        "print(\"Acuracy = \", sklearn.metrics.accuracy_score(dev_labels, test_predicted_labels), \"\\n\")\n",
        "\n",
        "print(\"Filter the dev data but not the training data:\")\n",
        "model = KNeighborsClassifier(n_neighbors=1)\n",
        "model.fit(mini_train_data, mini_train_labels)\n",
        "test_predicted_labels = model.predict(dev_data_blur)\n",
        "print(\"Acuracy = \", sklearn.metrics.accuracy_score(dev_labels, test_predicted_labels), \"\\n\")\n",
        "\n",
        "print(\"Filter both training data and dev data:\")\n",
        "model = KNeighborsClassifier(n_neighbors=1)\n",
        "model.fit(mini_train_data_blur, mini_train_labels)\n",
        "test_predicted_labels = model.predict(dev_data_blur)\n",
        "print(\"Acuracy = \", sklearn.metrics.accuracy_score(dev_labels, test_predicted_labels), \"\\n\")\n",
        "\n",
        "print(\"I got the highest accuracy by applying the blur to the training and test sets.\")\n",
        "print(\"The highest accuracy that I could achieve with the Gaussian blur is 0.896. (Slighty less than 0.90).\")\n",
        "print(\"Using a different filter, median_filter, I was able to achieve accuracy of 0.905. \\n\")\n",
        "\n",
        "\n",
        "mini_train_data_blur = np.array([ndimage.median_filter(x, 2) for x in mini_train_data])\n",
        "dev_data_blur = np.array([ndimage.median_filter(x, 2) for x in dev_data])\n",
        "\n",
        "#Filter both training data and dev data\n",
        "model = KNeighborsClassifier(n_neighbors=1)\n",
        "model.fit(mini_train_data_blur, mini_train_labels)\n",
        "test_predicted_labels = model.predict(dev_data_blur)\n",
        "print(\"Median filter applied to both training data and dev data:\")\n",
        "print(sklearn.metrics.accuracy_score(dev_labels, test_predicted_labels))\n",
        "print(\"\\n\")\n",
        "#Setup the plot grid and stlying \n",
        "f, axarr = plt.subplots(10, 10,figsize=(7,7))\n",
        "plt.set_cmap('binary')\n",
        "plt.setp(axarr, xticks=[], yticks=[])\n",
        "plt.suptitle('Sample digits with median filter applied', y=0.92, fontsize=14)\n",
        "\n",
        "#Iterate through each of the 10 digits\n",
        "for d in range(10):\n",
        "    n = 0\n",
        "    #Iterate through the scanned digits\n",
        "    for i in range(len(mini_train_labels)):\n",
        "        #l stands for lable, which we want as an int\n",
        "        l = int(mini_train_labels[i])\n",
        "        #We want 10 samples of each digit\n",
        "        #If the scanned digit matches the value we want for this row, render that item\n",
        "        if n < 10 and l==d:\n",
        "            axarr[d, n].imshow(mini_train_data_blur[i].reshape(28,28))\n",
        "            n += 1\n",
        "\n",
        "print(\"The median filter seems to create more of a thickening effect than a blur effect.\")\n",
        "\n",
        "### STUDENT END ###\n",
        "\n",
        "#P6()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LtgepWfAhWAk"
      },
      "source": [
        "### Part 7:\n",
        "\n",
        "Produce two Naive Bayes models and evaluate their performances.  Recall that Naive Bayes estimates P(feature|label), where each label is a categorical, not a real number.\n",
        "\n",
        "For the first model, map pixel values to either 0 or 1, representing white or black - you should pre-process the data or use `BernoulliNB`'s `binarize` parameter to set the white/black separation threshold to 0.1.  Use `BernoulliNB` to produce the model.\n",
        "\n",
        "For the second model, map pixel values to either 0, 1, or 2, representing white, gray, or black - you should pre-process the data, seting the white/gray/black separation thresholds to 0.1 and 0.9.  Use `MultinomialNB` to produce the model. \n",
        "\n",
        "Show the Bernoulli model accuracy and the Multinomial model accuracy.\n",
        "\n",
        "Notes:\n",
        "* Train on the mini train set.\n",
        "* Evaluate performance on the dev set.\n",
        "* `sklearn`'s Naive Bayes methods can handle real numbers, but for this exercise explicitly do the mapping to categoricals. \n",
        "\n",
        "Does the multinomial version improve the results? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eGpH-4IQhWAk",
        "colab": {}
      },
      "source": [
        "#def P7():\n",
        "\n",
        "### STUDENT START ###\n",
        "\n",
        "#Use Use BernoulliNB to produce model 1\n",
        "model_Multinomial =  MultinomialNB(alpha=0.001)\n",
        "model_Bernoulli =  BernoulliNB(alpha=0.001)\n",
        "\n",
        "model_Bernoulli.fit(mini_train_data, mini_train_labels)\n",
        "dev_predicted_labels = model_Bernoulli.predict(dev_data)\n",
        "BernoulliNB_accuracy = sklearn.metrics.accuracy_score(dev_labels, dev_predicted_labels)\n",
        "print(\"BernoulliNB_accuracy:\", BernoulliNB_accuracy)\n",
        "\n",
        "#print(\"\\nBinomial confusion matrix:\")\n",
        "Bernoulli_confusion_matrix = confusion_matrix(dev_labels, dev_predicted_labels)\n",
        "#print(Bernoulli_confusion_matrix)\n",
        "#print(classification_report(dev_labels, dev_predicted_labels))\n",
        "\n",
        "#Use MultinomialNB to produce the second model \n",
        "#Create a function to map pixel values to either 0, 1, or 2, representing white, gray, or black \n",
        "def greyscale(x):\n",
        "  if x <= 0.1:\n",
        "    return 0\n",
        "  elif (x <= 0.9):\n",
        "    return 1\n",
        "  else:\n",
        "    return 2\n",
        "\n",
        "mini_train_data_multinomial = np.empty(mini_train_data.shape)\n",
        "for i in range(len(mini_train_data)):\n",
        "  mini_train_data_multinomial[i] = np.array([greyscale(x) for x in mini_train_data[i]])\n",
        "\n",
        "model_Multinomial.fit(mini_train_data_multinomial, mini_train_labels)\n",
        "dev_predicted_labels = model_Multinomial.predict(dev_data)\n",
        "Multinomial_accuracy = sklearn.metrics.accuracy_score(dev_labels, dev_predicted_labels)\n",
        "print(\"\\nMultinomial_accuracy:\", Multinomial_accuracy)\n",
        "\n",
        "#print(\"\\nMultinomial confusion matrix:\")\n",
        "Multinomial_confusion_matrix = confusion_matrix(dev_labels, dev_predicted_labels)\n",
        "#print(Multinomial_confusion_matrix)\n",
        "#print(classification_report(dev_labels, dev_predicted_labels))\n",
        "\n",
        "#print(Bernoulli_confusion_matrix - Multinomial_confusion_matrix)\n",
        "print(\"\\nThis matrix shows the difference between the Bernoulli_confusion_matrix and the Multinomial_confusion_matrix.\")\n",
        "import seaborn as sns\n",
        "sns.heatmap((Bernoulli_confusion_matrix - Multinomial_confusion_matrix), square=True, annot=True, cbar=False)\n",
        "plt.xlabel('predicted value')\n",
        "plt.ylabel('true value');\n",
        "\n",
        "### STUDENT END ###\n",
        "\n",
        "#P7()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zNLrgggohWAm"
      },
      "source": [
        "ANSWER: I get higher accuracy from the Binomial model (0.814) than the Multinomial model (0.807). The accuracies are very similar but the multinomial model does not perform better. I created a matrix that shows difference in the confusion matrices between the two models. It shows that the two models actually performed differently though the end accuracy was similar. For example looking at the row for the digit 1, the multimodal mdel was more likely to misclassify the number 1 as an 8, and the number 4 as a 9. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PqjbRLg7hWAm"
      },
      "source": [
        "### Part 8:\n",
        "\n",
        "Search across several values of the LaPlace smoothing parameter (alpha) to find its effect on a Bernoulli Naive Bayes model's performance.  Show the accuracy at each alpha value.\n",
        "\n",
        "Notes:\n",
        "* Set binarization threshold to 0.\n",
        "* Train on the mini train set.\n",
        "* Evaluate performance by 5-fold cross-validation. \n",
        "* Use `GridSearchCV(..., ..., cv=..., scoring='accuracy', iid=False)` to vary alpha and evaluate performance by cross-validation.\n",
        "* Cross-validation is based on partitions of the training data, so results will be a bit different than if you had used the dev set to evaluate performance.\n",
        "\n",
        "What is the best value for alpha? What is the accuracy when alpha is near 0? Is this what you'd expect?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0AvZ-Wp3hWAn",
        "colab": {}
      },
      "source": [
        "#def P8(alphas):\n",
        "\n",
        "### STUDENT START ###\n",
        "alphas = {'alpha': [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
        "\n",
        "nb = GridSearchCV(BernoulliNB(binarize=0.0), alphas, cv=5, scoring='accuracy')\n",
        "nb.fit(mini_train_data,mini_train_labels) # running the grid search\n",
        "print(\"\\nBest alpha = \", nb.best_params_)\n",
        "print(\"Best score = \", nb.best_score_)\n",
        "print(\"Accuracy of alphas near 0:\", nb.cv_results_['mean_test_score'][0:2])\n",
        "### STUDENT END ###\n",
        "\n",
        "# alphas = {'alpha': [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
        "# nb = P8(alphas)\n",
        "# print()\n",
        "# print(\"Best alpha = \", nb.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1yEg9keThWAp"
      },
      "source": [
        "ANSWER: The best value for alpha is 0.001, which is the third smallest one. The smaller alphas closer to zero (1.0e-10, 0.0001) have accuracies of 0.814 and 0.819 respectively. This is the same value that we got for the Bernoulli model in question 7, which also had an alpha of 0 (because it was not defined). But that is not what I expect because the model in question 7 had a binarize threshold of 0.1, so I thought that would make it perform better. But it seems that an alpha of 0.001 has the same effect as a binary threshold of 0.1. I did a test and combine an alpha of 0.001 and threshold of 0.1 and the accuracy went up to 0.829. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B07GDiDdhWAq"
      },
      "source": [
        "### Part 9:\n",
        "\n",
        "Produce a model using Guassian Naive Bayes, which is intended for real-valued features, and evaluate performance. You will notice that it does not work so well. Diagnose the problem and apply a simple fix so that the model accuracy is around the same as for a Bernoulli Naive Bayes model. Show the model accuracy before your fix and the model accuracy after your fix.  Explain your solution.\n",
        "\n",
        "Notes:\n",
        "* Train on the mini train set.\n",
        "* Evaluate performance on the dev set.\n",
        "* Consider the effects of theta and sigma.  These are stored in the model's `theta_` and `sigma_` attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gBLbTMWChWAq",
        "colab": {}
      },
      "source": [
        "#def P9():\n",
        "\n",
        "### STUDENT END ###\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(mini_train_data, mini_train_labels)\n",
        "test_predicted_labels = model.predict(dev_data)\n",
        "print(\"Model 1 Acuracy = \", sklearn.metrics.accuracy_score(dev_labels, test_predicted_labels) , \"\\n\")\n",
        "plt.hist(model.theta_[1])\n",
        "plt.title('Theta Histogram of 1s')\n",
        "plt.show()\n",
        "plt.hist(model.sigma_[1])\n",
        "plt.title('Sigma Histogram of 1s')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.Normalizer()\n",
        "mini_train_data_scaled = scaler.fit_transform(mini_train_data)\n",
        "dev_data_scaled = scaler.fit_transform(dev_data)\n",
        "\n",
        "model2 = GaussianNB(var_smoothing=0.1)\n",
        "model2.fit(mini_train_data_scaled, mini_train_labels)\n",
        "test_predicted_labels = model2.predict(dev_data_scaled)\n",
        "print(\"\\nModel 2 accuracy = \", sklearn.metrics.accuracy_score(dev_labels, test_predicted_labels) , \"\\n\")\n",
        "\n",
        "plt.hist(model2.theta_[1])\n",
        "plt.title('Theta Histogram of 1s')\n",
        "plt.show()\n",
        "plt.hist(model2.sigma_[1])\n",
        "plt.title('Sigma Histogram of 1s')\n",
        "plt.show()\n",
        "\n",
        "### STUDENT END ###\n",
        "\n",
        "#P9()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1SyHTEJohWAt"
      },
      "source": [
        "***ANSWER***: The performance of the basic model was  0.59 and I improved it to 0.754 but I am not sure if I made the improvement you were looking for. I Checked the Theta and Sigma of each model and they don't look very different. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dgZMuc1VhWAt"
      },
      "source": [
        "### Part 10:\n",
        "\n",
        "Because Naive Bayes produces a generative model, you can use it to generate digit images.\n",
        "\n",
        "Produce a Bernoulli Naive Bayes model and then use it to generate a 10x20 grid with 20 example images of each digit. Each pixel output should be either 0 or 1, based on comparing some randomly generated number to the estimated probability of the pixel being either 0 or 1.  Show the grid.\n",
        "\n",
        "Notes:\n",
        "* You can use np.random.rand() to generate random numbers from a uniform distribution.\n",
        "* The estimated probability of each pixel being 0 or 1 is stored in the model's `feature_log_prob_` attribute. You can use `np.exp()` to convert a log probability back to a probability.\n",
        "\n",
        "How do the generated digit images compare to the training digit images?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktii-Mp-hWAu",
        "colab": {}
      },
      "source": [
        "#def P10(num_examples):\n",
        "\n",
        "### STUDENT START ###\n",
        "\n",
        "#Produce a Bernoulli Naive Bayes model\n",
        "model_Bernoulli = BernoulliNB(binarize=0.0)\n",
        "model_Bernoulli.fit(mini_train_data, mini_train_labels)\n",
        "\n",
        "#The estimated probability of each pixel being 0 or 1 is stored in the model's feature_log_prob_ attribute\n",
        "feature_prob = np.exp(model_Bernoulli.feature_log_prob_)\n",
        "\n",
        "#generate a 10x20 grid with 20 example images of each digit\n",
        "f, axarr = plt.subplots(10, 20,figsize=(20,20))\n",
        "plt.set_cmap('binary')\n",
        "plt.setp(axarr, xticks=[], yticks=[])\n",
        "plt.suptitle('Sample digits generated from trained Naive Bayes model', y=0.92, fontsize=20)\n",
        "\n",
        "for d in range(10):\n",
        "  for i in range(20):\n",
        "    generated_image = np.random.binomial(1, feature_prob[d])\n",
        "    axarr[d, i].imshow(generated_image.reshape(28,28))\n",
        "\n",
        "\n",
        "### STUDENT END ###\n",
        "\n",
        "#P10(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SuQd1fTGhWAw"
      },
      "source": [
        "ANSWER: The samples digits are all basically the same shape. For example the 7s are all basically the same. The variations is in the amount of \"noise\", random pixels in the grid. The basic shape is basically an average of all the samples and the noise is random based on the probability of something being in that pixel. What the model does not consider is the conditional probability based on other pixels. In the samples there are very few free floating pixels, but the model does not understand that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ksHMg73uhWAx"
      },
      "source": [
        "### Part 11:\n",
        "\n",
        "Recall that a strongly calibrated classifier is rougly 90% accurate when the posterior probability of the predicted class is 0.9. A weakly calibrated classifier is more accurate when the posterior probability of the predicted class is 90% than when it is 80%. A poorly calibrated classifier has no positive correlation between posterior probability and accuracy.  \n",
        "\n",
        "Produce a Bernoulli Naive Bayes model.  Evaluate performance: partition the dev set into several buckets based on the posterior probabilities of the predicted classes - think of a bin in a histogram- and then estimate the accuracy for each bucket. So, for each prediction, find the bucket to which the maximum posterior probability belongs, and update \"correct\" and \"total\" counters accordingly.  Show the accuracy for each bucket.\n",
        "\n",
        "Notes:\n",
        "* Set LaPlace smoothing (alpha) to the optimal value (from part 8).\n",
        "* Set binarization threshold to 0.\n",
        "* Train on the mini train set.\n",
        "* Evaluate perfromance on the dev set.\n",
        "\n",
        "How would you characterize the calibration for this Bernoulli Naive Bayes model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a1N-St12hWAy",
        "colab": {}
      },
      "source": [
        "#def P11(buckets, correct, total):\n",
        "    \n",
        "### STUDENT START ###\n",
        "\n",
        "buckets = [0.5, 0.9, 0.999, 0.99999, 0.9999999, 0.999999999, 0.99999999999, 0.9999999999999, 1.0]\n",
        "\n",
        "#Produce a Bernoulli Naive Bayes model.\n",
        "#Set LaPlace smoothing (alpha) to the optimal value (from part 8)\n",
        "#Set binarization threshold to 0.\n",
        "model = BernoulliNB(alpha=0.001, binarize=0)\n",
        "#Train on the mini train set.\n",
        "model.fit(mini_train_data,mini_train_labels) \n",
        "#Evaluate perfromance on the dev set.\n",
        "dev_data_probs = model.predict_proba(dev_data)\n",
        "\n",
        "dev_data_maxprob = []\n",
        "dev_data_maxprediction = []\n",
        "\n",
        "#for each prediction, find the bucket to which the maximum posterior probability belongs\n",
        "for i in range(len(dev_data_probs)):\n",
        "  dev_data_maxprob.append(max(dev_data_probs[i]))\n",
        "  dev_data_maxprediction.append(np.argmax(dev_data_probs[i]))\n",
        "dev_data_buckets = np.digitize(dev_data_maxprob, buckets)\n",
        "\n",
        "#Put the score, labels, and predictions into a table for easy calculation \n",
        "import pandas as pd\n",
        "prob_buckets = pd.DataFrame()\n",
        "prob_buckets[\"Buckets\"] = dev_data_buckets\n",
        "prob_buckets[\"Prediction\"] = dev_data_maxprediction\n",
        "prob_buckets[\"TrueLabel\"] = dev_labels\n",
        "prob_buckets[\"TrueLabel\"] = prob_buckets[\"TrueLabel\"].astype(int)\n",
        "prob_buckets[\"Correct\"] = prob_buckets.Prediction == prob_buckets.TrueLabel\n",
        "prob_buckets[\"Correct\"] = prob_buckets[\"Correct\"].astype(int)\n",
        "prob_buckets[\"Total\"] = 1\n",
        "\n",
        "#Summarize by bucket\n",
        "prob_buckets_groups = prob_buckets.groupby(\"Buckets\").sum()\n",
        "prob_buckets_groups = prob_buckets_groups.drop([\"TrueLabel\", \"Prediction\"], axis=1)\n",
        "prob_buckets_groups.insert(0,'Probability', buckets)\n",
        "prob_buckets_groups[\"Probability\"] = prob_buckets_groups[\"Probability\"].astype(str)\n",
        "prob_buckets_groups[\"Accuracy\"] = round(prob_buckets_groups[\"Correct\"] / prob_buckets_groups[\"Total\"], 3)\n",
        "\n",
        "plt.title(\"Accuracy of prediction vs confidence of prediction\")\n",
        "plt.plot(prob_buckets_groups[\"Probability\"], prob_buckets_groups[\"Accuracy\"])\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Confidence of prediction\")\n",
        "plt.ylabel(\"Accuracy of prediction\")\n",
        "plt.grid(color='gray', linestyle='-', linewidth=1)\n",
        "\n",
        "(prob_buckets_groups)\n",
        "\n",
        "### STUDENT END ###\n",
        "\n",
        "# P11(buckets, correct, total)\n",
        "\n",
        "# for i in range(len(buckets)):\n",
        "#     accuracy = 0.0\n",
        "#     if (total[i] > 0): accuracy = correct[i] / total[i]\n",
        "#     print('p(pred) is %.13f to %.13f    total = %3d    accuracy = %.3f' % (0 if i==0 else buckets[i-1], buckets[i], total[i], accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h-4qQsrrhWA1"
      },
      "source": [
        "ANSWER:I would categorize this as a weakly calibrated model. It is good in the sense that as the confidence improves the accuracy improves. And at the upper end when the model is 100% confident then the accuracy is 94%  "
      ]
    }
  ]
}